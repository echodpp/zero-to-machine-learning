{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the problem\n",
    "What is the purpose of the project, in the case of house price forecasting, then the purpose is to forecast the median property price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "- What is the business purpose, after all, building models is not the ultimate purpose\n",
    "- To know how effective the current solution is, for example, will give an error rate of the current solution is alpha\n",
    "- One can further study the problem to clarify whether the problem is supervised/unsupervised, or a reinforcement model? Is it classification/regression, or other such as clustering. To use batch learning or online learning?\n",
    "- Example: We have house price values, so it is a supervised problem; we eventually want to predict the median house price, so it is a regression problem, and it is a multivariate predictive regression because there are many influencing parameters; in addition, there is no continuous inflow of data, and there is no special need to make quick adaptation to data changes. The amount of data is not large enough to be put into memory, so batch learning is fine. If the data volume is large, you can either split the batch learning across multiple servers (using MapReduce techniques, as you will see later), or use online learning]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select performance indicators\n",
    "Here we need to choose an evaluation metric, typical for regression problems is the root mean squared error RMSE, which characterizes the standard deviation of the system prediction error.\n",
    "Alternatively, the difference squared absolute error can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check assumptions\n",
    "- Check the data, for example, check the data type, check the missing data, check the outliers, check the distribution of the data, check the correlation between the data, check the data quality, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create workspace\n",
    "\n",
    "such as python jupyter and corresponding library files (such as numpy, pandas, scipy, and sklearn, etc.) and frameworks (tf, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "- Download the data\n",
    "- Load the data\n",
    "- Explore the data\n",
    "- Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('housing.csv')\n",
    "data.head()\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "import altair as alt\n",
    "alt.chart(data).mark_point().encode(\n",
    "    x='longitude',\n",
    "    y='latitude',\n",
    "    color='median_house_value:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a test set\n",
    "\n",
    "Before viewing the data, it is best to create a test set so that the selection of the test set is not affected by the mindset after viewing the data.\n",
    "One way is that you can choose the test set randomly, for example, choose 20% of the data randomly as the test set, but then when the data set is updated, the test set will change and we can use random numbers to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, there is a risk of losing the distribution of key features by random sampling. For example, there is a feature A that contributes a lot to the final label (the correlation between them is strong).\n",
    "Then we should also ensure in the test set that the distribution of A follows the distribution trend of the original dataset. This can be done using stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data[\"A_new\"] = np.ceil(data[\"A\"] / 1.5)\n",
    "data[\"A_new\"].where(data[\"A_new\"] < 5, 5.0, inplace=True)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(data, data[\"A_new\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "\n",
    "#The results of stratified sampling can be checked with the following code.\n",
    "data[\"A_new\"].value_counts() / len(data)\n",
    "strat_test_set[\"A_new\"].value_counts() / len(strat_test_set)\n",
    "\n",
    "# Note that the generated A_new tag needs to be removed at the end, with the drop command, code.\n",
    "for set in (strat_train_set, strat_test_set):\n",
    "    set.drop([\"A_new\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we generated two training-test sets, random and hierarchical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Frequently, we need to first take a look at the data and can determine how the features relate to the labels and which features are more useful or influential. The matplotlib library can be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")#Scatterplot to see the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding correlations\n",
    "\n",
    "You can use the corr() method to calculate the standard correlation coefficient between each pair of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_m=data.corr()\n",
    "print(corr_m['median_house_value'].sort_values(ascending=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that corr() can only characterize linear relationships and ignore non-linear relationships, so there are limitations in its referential properties. There is another way to detect correlation coefficients between attributes when pandas' scatter_matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "pd.plotting.scatter_matrix(data[attributes], figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute combination test\n",
    "\n",
    "Sometimes just using the original feature data does not work well, it is possible to consider combining some features to generate new features, such as the number of people / households, to get a feature such as the number of people per household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['population_per_household']=data['population']/data['households']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for machine learning algorithms\n",
    "Don't do it by hand, you need to write some functions for the following reasons.\n",
    "Functions allow you to easily perform repetitive data transformations on any dataset (for example, the next time you fetch a new dataset).\n",
    "You can slowly build a library of transformation functions that can be reused in future projects.\n",
    "You can use these functions in your real-time system before passing the data to the algorithm.\n",
    "This allows you to easily try multiple data conversions and see which conversion methods work best in combination.\n",
    "One thing to keep in mind is to always remember to replicate the data to ensure that later data processing does not affect the initial data as much as possible. Do the markup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Problems such as missing values can exist in the original data, so the data needs to be cleaned. This is a very critical step.\n",
    "There are three ways to deal with missing values\n",
    "1. directly delete the row where the missing value is located; \n",
    "2. if a feature has too many missing values, then directly delete the feature; \n",
    "3. assign a value to the missing position (with 0, median or average, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(housing['total_bedrooms'])\n",
    "housing.drop('total_bedrooms',axis=1)\n",
    "housing['total_bedrooms'].fillna(median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use the third way so that you can make full use of the original data. The Imputer class of sklearn can be used to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text and category attributes\n",
    "\n",
    "There will be some text types in the data, which we can recode using one-hot when processing them, which requires two transformations (text category to integer category and then to one-hot vector)\n",
    "These two transformations can be implemented with sklearn's LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "print(housing_cat_1hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above classes should also be used for label column conversions, and the correct approach is to use the CategoricalEncoder class that sklearn will soon provide, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = CategoricalEncoder()\n",
    "housing_cat_reshaped = housing_cat.values.reshape(-1, 1)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "print(housing_cat_1hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom converters\n",
    "\n",
    "The role of converters is to perform some data processing operations together, such as cleaning, attribute combination, etc. as described earlier, in addition to the homemade converters that can work seamlessly with sklearn's pipeline. The sample code for this section can be found in the documentation you wrote (note:). This part can have the property combinations written in it.\n",
    "Note that here you can set some hyperparameters for the attribute to check if this attribute is helpful for the ground ML algorithm.\n",
    "\n",
    "Feature scaling\n",
    "\n",
    "This step is important for the different problems of inputting numerical attribute measures. For example, if the age attribute is in the range of 20 to 50, and the income distribution is in the range of 5000 to 100000, the performance of such data applied to the algorithm will not be too good. Usually do not scale the target values.\n",
    "Two ways.\n",
    "Linear function normalization (min-max-scaling) - subtract the minimum value and divide by the difference between the maximum and minimum values, sklearn's MinMaxScaler\n",
    "Standardization - subtract the mean and divide by the variance, resulting in a distribution with unit variance. sklearn's StandardScaler\n",
    "Note:All operations such as data conversion should be applied to the training and test sets separately, not to the completed data set.\n",
    "\n",
    "Transformation pipeline\n",
    "\n",
    "The purpose of a pipeline is to create a pattern that allows data to be processed and transformed in a certain order. For example, the following is a complete pipeline for processing numeric and category attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared=full_pipeline.fit_transform(housing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls the steps: num_pipeline->DataFrameSelector->Imputer->CombinedAttributesAdder->StandardScaler->cat_pipeline->DataFrameSelector-> CategoricalEncoder to get the processed training set.\n",
    "The representations are: sub-pipeline data manipulation -> selection converter -> missing value processing -> attribute combination -> normalization -> sub-pipeline categorization processing -> selection converter -> categorical marker as one-hot vector\n",
    "Explanation for the selection converter: Converts the data by selecting the corresponding attributes (values or categories), discarding the rest, and turning the output DataFrame into a NumPy array. scikit-Learn does not have tools to handle PandasDataFrame, so we need to write a simple custom converter to do the job:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and train models\n",
    "\n",
    "Training and evaluation on the training set\n",
    "\n",
    "Here we can select the algorithmic model to train the data for learning (in fact, we can find that most of the work is focused on the pre-processing of the data, including cleaning visualization text class attribute transformation, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "display_scores(tree_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not only linear regression, but also other models such as decision tree model and random forest model can be used, and the steps are the same as above.\n",
    "\n",
    "Using cross-validation for better evaluation\n",
    "\n",
    "Alternatively, we can use cross-validation to validate the model, using decision trees as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the training set is randomly divided into ten different subsets, which become \"folds\", and then the evaluation decision tree model is trained 10 times, each time one unused fold is selected for evaluation and the other 9 are used for training. The result is an array of 10 scores.\n",
    "The Scikit-Learn cross-validation function expects a utility function (bigger is better) rather than a loss function (lower is better), so the score function is actually the opposite of the MSE (i.e., negative), which is why the previous code calculates -scores before calculating the square root.\n",
    "Results view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning\n",
    "\n",
    "Grid Search\n",
    "\n",
    "Use Scikit-Learn's GridSearchCV method. to target random forests as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearch\n",
    "\n",
    "When the search space for hyperparameters is large, it is best to use RandomizedSearchCV. this class is used much like the class GridSearchCV, but instead of trying all possible combinations, it does so by selecting a specific number of random combinations of a random value for each hyperparameter.\n",
    "\n",
    "Integration methods\n",
    "\n",
    "Another way to fine-tune the system is to combine the best-performing models.\n",
    "\n",
    "Analyzing the best models and their errors\n",
    "\n",
    "A deeper understanding of the problem can often be gained by analyzing the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(encoder.classes_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances,attributes), reverse=True)\n",
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) \n",
    "print(final_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above importance scores, we can discard some unimportant attributes, etc.\n",
    "\n",
    "Evaluate the system with test sets\n",
    "\n",
    "After finally debugging the model, we need to test it with the test set. Note that the test set has been useless after we split the data before, so we need to process the test set first, such as dropping labels, pipelining, etc. Then we apply our model to it. Then we can apply our model to it.\n",
    "\n",
    "\n",
    "At this point, we have basically finished creating and testing the model, and then we need to present the results or conclusions.\n",
    "\n",
    "Start, monitor and maintain the system\n",
    "\n",
    "The above models are embedded into the company's system for automated operation.\n",
    "\n",
    "Practice\n",
    "\n",
    "Don't waste on advanced algorithms, just be able to use them. The focus is on understanding the business and data, and the processing of the data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('dpp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "849d6c3c8775fcd616d7006a9080f326098ff42752053d04d35a2efce1d0215c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
