{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the problem\n",
    "What is the purpose of the project, in the case of house price forecasting, then the purpose is to forecast the median property price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "- What is the business purpose, after all, building models is not the ultimate purpose\n",
    "- To know how effective the current solution is, for example, will give an error rate of the current solution is alpha\n",
    "- One can further study the problem to clarify whether the problem is supervised/unsupervised, or a reinforcement model? Is it classification/regression, or other such as clustering. To use batch learning or online learning?\n",
    "- Example: We have house price values, so it is a supervised problem; we eventually want to predict the median house price, so it is a regression problem, and it is a multivariate predictive regression because there are many influencing parameters; in addition, there is no continuous inflow of data, and there is no special need to make quick adaptation to data changes. The amount of data is not large enough to be put into memory, so batch learning is fine. If the data volume is large, you can either split the batch learning across multiple servers (using MapReduce techniques, as you will see later), or use online learning]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select performance indicators\n",
    "Here we need to choose an evaluation metric, typical for regression problems is the root mean squared error RMSE, which characterizes the standard deviation of the system prediction error.\n",
    "Alternatively, the difference squared absolute error can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check assumptions\n",
    "- Check the data, for example, check the data type, check the missing data, check the outliers, check the distribution of the data, check the correlation between the data, check the data quality, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create workspace\n",
    "\n",
    "such as python jupyter and corresponding library files (such as numpy, pandas, scipy, and sklearn, etc.) and frameworks (tf, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "- Download the data\n",
    "- Load the data\n",
    "- Explore the data\n",
    "- Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('housing.csv')\n",
    "data.head()\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a test set\n",
    "\n",
    "Before viewing the data, it is best to create a test set so that the selection of the test set is not affected by the mindset after viewing the data.\n",
    "One way is that you can choose the test set randomly, for example, choose 20% of the data randomly as the test set, but then when the data set is updated, the test set will change and we can use random numbers to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, there is a risk of losing the distribution of key features by random sampling. For example, there is a feature A that contributes a lot to the final label (the correlation between them is strong).\n",
    "Then we should also ensure in the test set that the distribution of A follows the distribution trend of the original dataset. This can be done using stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"A_new\"] = np.ceil(data[\"A\"] / 1.5)\n",
    "data[\"A_new\"].where(data[\"A_new\"] < 5, 5.0, inplace=True)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(data, data[\"A_new\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "\n",
    "#The results of stratified sampling can be checked with the following code.\n",
    "data[\"A_new\"].value_counts() / len(data)\n",
    "strat_test_set[\"A_new\"].value_counts() / len(strat_test_set)\n",
    "\n",
    "# Note that the generated A_new tag needs to be removed at the end, with the drop command, code.\n",
    "for set in (strat_train_set, strat_test_set):\n",
    "    set.drop([\"A_new\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Frequently, we need to first take a look at the data and can determine how the features relate to the labels and which features are more useful or influential. The matplotlib library can be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding correlations\n",
    "\n",
    "You can use the corr() method to calculate the standard correlation coefficient between each pair of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_m=housing.corr()\n",
    "print(corr_m['median_house_value'].sort_values(ascending=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that corr() can only characterize linear relationships and ignore non-linear relationships, so there are limitations in its referential properties. There is another way to detect correlation coefficients between attributes when pandas' scatter_matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute combination test\n",
    "\n",
    "Sometimes just using the original feature data does not work well, it is possible to consider combining some features to generate new features, such as the number of people / households, to get a feature such as the number of people per household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['population_per_household']=housing['population']/housing['households']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Problems such as missing values can exist in the original data, so the data needs to be cleaned. This is a very critical step.\n",
    "There are three ways to deal with missing values\n",
    "1. directly delete the row where the missing value is located; \n",
    "2. if a feature has too many missing values, then directly delete the feature; \n",
    "3. assign a value to the missing position (with 0, median or average, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(housing['total_bedrooms'])\n",
    "housing.drop('total_bedrooms',axis=1)\n",
    "housing['total_bedrooms'].fillna(median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "print(housing_cat_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = CategoricalEncoder()\n",
    "housing_cat_reshaped = housing_cat.values.reshape(-1, 1)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "print(housing_cat_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared=full_pipeline.fit_transform(housing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "from from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "...     print(\"Scores:\", scores)\n",
    "...     print(\"Mean:\", scores.mean())\n",
    "...     print(\"Standard deviation:\", scores.std())\n",
    "display_scores(tree_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(encoder.classes_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances,attributes), reverse=True)\n",
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) \n",
    "print(final_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('dpp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "849d6c3c8775fcd616d7006a9080f326098ff42752053d04d35a2efce1d0215c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
